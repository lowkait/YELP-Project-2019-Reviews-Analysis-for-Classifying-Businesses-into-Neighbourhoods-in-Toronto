{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no letter at end = after kNearest was run<br>\n",
    "a is after official neighbourhoods by Id was created <br>\n",
    "b is after errors list was created, Group 1 to 4 <br>\n",
    "c is after accuracy was updated <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-b39010dcac6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"YELP-Test-650-1-c.pkl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\dill\\dill.py\u001b[0m in \u001b[0;36mload_session\u001b[1;34m(filename, main)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_main\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mmain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "filename = \"YELP-Test-650-1-c.pkl\"\n",
    "dill.load_session(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"YELP-Functions.py\"\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# # YELP Project 2019: Reviews Analysis for Classifying Businesses into Neighbourhoods in Toronto\n",
    "# ### Overall Question(s): Can language distinguish groups of businesses/people? What in the language? Why?\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from tqdm import tqdm\n",
    "from collections import *\n",
    "import operator\n",
    "import itertools\n",
    "import dill\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import descartes\n",
    "from shapely.geometry import Point, Polygon\n",
    "import math\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "def everyWord(file, stop_words):\n",
    "    everyWord_counter = Counter()\n",
    "    for comment in tqdm(file['1'].values):\n",
    "        comment = [t.lower() for t in comment.replace('.',' ').replace(',',' ').split(\" \") if ((len(t) > 1) and (t.lower() not in stop_words))]\n",
    "        everyWord_counter.update(comment)\n",
    "    return everyWord_counter\n",
    "\n",
    "def topktermFreq(everyWord, stop_words, k):\n",
    "    AllWords_stop = Counter(everyWord)\n",
    "    sorted_dict = OrderedDict(sorted(AllWords_stop.items(), key=operator.itemgetter(1), reverse=True))\n",
    "    topktermFreq = dict(itertools.islice(sorted_dict.items(), k))\n",
    "    return topktermFreq\n",
    "\n",
    "def topkdocFreq(file, stop_words, k):\n",
    "    allWordsinFile = everyWord(file, stop_words)\n",
    "    topkTF = topktermFreq(allWordsinFile, stop_words, k)\n",
    "    topkdocFreq = defaultdict(int)\n",
    "    for comment in tqdm(file['1'].values):\n",
    "        comment = set(t.lower() for t in comment.replace('.',' ').replace(',',' ').split(\" \") if len(t) > 1 and (t.lower() not in stop_words))\n",
    "        for word in comment:\n",
    "            if word in topkTF.keys():\n",
    "                topkdocFreq[word] += 1\n",
    "            else:\n",
    "                continue\n",
    "    return topkdocFreq\n",
    "\n",
    "def TFIDF_xtrain(xtrain_file, topkDF, stop_words):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    # for each of the comments (rows)\n",
    "    row_index = 0\n",
    "    # loop through each of the comments in the \n",
    "    for comment in tqdm(xtrain_file['1'].values):\n",
    "        #comment contains all the words in the comments, but we are only interested in the 15\n",
    "        comment = [t.lower() for t in comment.replace('.',' ').replace(',',' ').split(\" \") if ((len(t) > 1) and (t.lower() not in stop_words))]\n",
    "        # create a dictionary for all words\n",
    "        c_counter = Counter(comment)        \n",
    "        col_index = 0\n",
    "        # loop through the top 10k words\n",
    "        for word in topkDF.keys():\n",
    "            if word in c_counter.keys():\n",
    "                row.append(row_index)\n",
    "                col.append(col_index)\n",
    "                data.append(round(((c_counter[word]/len(comment))*math.log10(len(xtrain_file)/topkDF[word])),5))\n",
    "                col_index += 1\n",
    "            else:\n",
    "                col_index += 1\n",
    "        row_index += 1\n",
    "    return sparse.coo_matrix((data,(row,col)), shape = (len(xtrain_file),len(topkDF))).toarray()\n",
    "\n",
    "def TFIDF_test(xtrain_file, test_file, topkDF, stop_words):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    # for each of the comments (rows)\n",
    "    row_index = 0\n",
    "    # loop through each of the comments in the \n",
    "    for comment in tqdm(test_file['1'].values):      \n",
    "        #comment contains all the words in the comments, but we are only interested in the 15\n",
    "        comment = [t.lower() for t in comment.replace('.',' ').replace(',',' ').split(\" \") if ((len(t) > 1) and (t.lower() not in stop_words))]\n",
    "        # create a dictionary for all words\n",
    "        c_counter = Counter(comment)  \n",
    "        col_index = 0\n",
    "        # loop through the top 10k words\n",
    "        for word in topkDF.keys():\n",
    "            if word in c_counter.keys():\n",
    "                row.append(row_index)\n",
    "                col.append(col_index)\n",
    "                data.append(round(((c_counter[word]/len(comment))*math.log10(len(xtrain_file)/topkDF[word])),5))\n",
    "                col_index += 1\n",
    "            else:\n",
    "                col_index += 1\n",
    "        row_index += 1\n",
    "    return sparse.coo_matrix((data,(row,col)), shape = (len(test_file),len(topkDF))).toarray()\n",
    "\n",
    "\n",
    "def existing_neighbourhood_dictionary(businesses_file, neighbourhoods_file):\n",
    "    id=[]\n",
    "    latitude=[]\n",
    "    longitude=[]\n",
    "    for x in range(businesses_file.shape[0]):\n",
    "        id.append(businesses_file.iloc[x,13])\n",
    "        longitude.append(businesses_file.iloc[x,30])\n",
    "        latitude.append(businesses_file.iloc[x,31])  \n",
    "    df = pd.DataFrame(\n",
    "        {'ID': id,\n",
    "         'Latitude': latitude,\n",
    "         'Longitude': longitude})\n",
    "    gdf = gpd.GeoDataFrame(df, geometry= gpd.points_from_xy(df.Longitude, df.Latitude))    \n",
    "    business_neighbourhood = {}\n",
    "    b_names = businesses_file.iloc[:,13]\n",
    "    for key in b_names:\n",
    "        business_neighbourhood[key] = float('nan')      \n",
    "    for i in range(gdf.shape[0]): #for businesses\n",
    "        for j in range(neighbourhoods_file.shape[0]): # for neighbourhood\n",
    "            if (neighbourhoods_file.loc[j, 'geometry']).contains(gdf.iloc[i,3]) == True:\n",
    "                business_neighbourhood[businesses_file.iloc[i,13]] = neighbourhoods_file.iloc[j,6]\n",
    "    return business_neighbourhood\n",
    "\n",
    "def official_neighbourhoods(businesses_file):\n",
    "    neighbourhood_official = {}\n",
    "    for x in tqdm(range(businesses_file.shape[0])):\n",
    "        neighbourhood_official[(businesses_file.iloc[x,13])] = businesses_file.iloc[x,29]\n",
    "    return neighbourhood_official\n",
    "\n",
    "def cosineSimilarity(tfidf_xtrain, tfidf_test):\n",
    "    similarities = np.zeros((len(tfidf_test),len(tfidf_xtrain)))\n",
    "    for i in tqdm(range(tfidf_test.shape[0])):\n",
    "        a = tfidf_test[i]\n",
    "        for j in range(tfidf_xtrain.shape[0]):\n",
    "                b = tfidf_xtrain[j]\n",
    "                cosinesimilarity = round(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)), 1)\n",
    "                if cosinesimilarity == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    similarities[i][j] = cosinesimilarity\n",
    "    return similarities\n",
    "\n",
    "def kNearestNeighbours(test_file, xtrain_comments_file, existing_neighbourhoods_xtrain, cos_sim, k): #want to specify the size  \n",
    "    kNearest = {}   \n",
    "    # Have a dictionary stating the business and the neighbourhood, for ex. {2: 'A', 3: 'C', 6: 'C'}\n",
    "    for row in tqdm(range(len(cos_sim))):\n",
    "        possible_neighbours = []\n",
    "        for col in range(len(cos_sim[0])):\n",
    "            possible_neighbours.append((cos_sim[row][col], existing_neighbourhoods_xtrain[xtrain_comments_file.iloc[col,1]]))\n",
    "        possible_neighbours.sort(key=lambda x: x[0], reverse=True)\n",
    "  \n",
    "        votes = Counter()\n",
    "        for index in range(k):\n",
    "            votes[possible_neighbours[index][1]] += 1\n",
    "    \n",
    "        kNearest[test_file.iloc[row,1]] = votes.most_common(1)[0][0]\n",
    "    return kNearest\n",
    "        \n",
    "def Accuracy(kNearest_, existing_neighbourhood_dictionary):\n",
    "    total = len(kNearest_)\n",
    "    sum = 0\n",
    "    for key in kNearest_.keys():\n",
    "        if kNearest_[key] == existing_neighbourhood_dictionary[key]:\n",
    "            sum += 1\n",
    "    average = round(sum/total,5)\n",
    "    return average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load YELP-files.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "stop_words =  list(stopwords.words('english')) \n",
    "businesses = pd.read_csv(\"businesses (1).csv\")\n",
    "data = pd.read_csv('business_text_stripped.csv')\n",
    "toronto_map = gpd.read_file('Neighbourhoods.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST CONDITIONS: <br>\n",
    "### test size = 0.33 (Xtrain = 21,454 ; Xtest = 10568)\n",
    "### 650 WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topkDF_650 = topkdocFreq(X_train, stop_words, 650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_xtrain_650 = TFIDF_xtrain(X_train, topkDF_650, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test_650 = TFIDF_test(X_train, X_test, topkDF_650, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_650 = cosineSimilarity(tfidf_xtrain_650, tfidf_test_650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32123/32123 [00:01<00:00, 18971.84it/s]\n"
     ]
    }
   ],
   "source": [
    "official_neighbourhoods_byName = official_neighbourhoods(businesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into 4 Test Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGroup_1, testGroup_2, testGroup_3, testGroup_4 = np.array_split(X_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_1, cos_sim_2, cos_sim_3, cos_sim_4 = np.array_split(cos_sim_650, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Group 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many are supposed to be Nan? 557 out of 2642 don't even belong to any of the neighbourhoods listed. How do we identify these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [39:37<00:00,  1.61it/s]  \n"
     ]
    }
   ],
   "source": [
    "kNearest_650_1 = kNearestNeighbours(testGroup_1, X_train, official_neighbourhoods_byName, cos_sim_1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_650_1 = Accuracy(kNearest_650_1, official_neighbourhoods_byName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0564"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_650_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 240105.55it/s]\n"
     ]
    }
   ],
   "source": [
    "errors_1 = []\n",
    "for key in tqdm(kNearest_650_1.keys()):\n",
    "    if kNearest_650_1[key] != official_neighbourhoods_byName[key] and isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        errors_1.append((\"Prediction: \" + str(kNearest_650_1[key]) +  \" but actually: \" + official_neighbourhoods_byName[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [24:23<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "kNearest_650_2 = kNearestNeighbours(testGroup_2, X_train, official_neighbourhoods_byName, cos_sim_2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_650_2 = Accuracy(kNearest_650_2, official_neighbourhoods_byName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05148"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_650_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 203197.05it/s]\n"
     ]
    }
   ],
   "source": [
    "errors_2 = []\n",
    "for key in tqdm(kNearest_650_2.keys()):\n",
    "    if kNearest_650_2[key] != official_neighbourhoods_byName[key] and isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        errors_2.append((\"Prediction: \" + str(kNearest_650_2[key]) +  \" but actually: \" + official_neighbourhoods_byName[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [27:52<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "kNearest_650_3 = kNearestNeighbours(testGroup_3, X_train, official_neighbourhoods_byName, cos_sim_3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_650_3 = Accuracy(kNearest_650_3, official_neighbourhoods_byName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04921"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_650_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 240121.15it/s]\n"
     ]
    }
   ],
   "source": [
    "errors_3 = []\n",
    "for key in tqdm(kNearest_650_3.keys()):\n",
    "    if kNearest_650_3[key] != official_neighbourhoods_byName[key] and isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        errors_3.append((\"Prediction: \" + str(kNearest_650_3[key]) +  \" but actually: \" + official_neighbourhoods_byName[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Group 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [27:45<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "kNearest_650_4 = kNearestNeighbours(testGroup_4, X_train, official_neighbourhoods_byName, cos_sim_4, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_650_4 = Accuracy(kNearest_650_4, official_neighbourhoods_byName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04693"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_650_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 220099.53it/s]\n"
     ]
    }
   ],
   "source": [
    "errors_4 = []\n",
    "for key in tqdm(kNearest_650_4.keys()):\n",
    "    if kNearest_650_4[key] != official_neighbourhoods_byName[key] and isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        errors_4.append((\"Prediction: \" + str(kNearest_650_4[key]) +  \" but actually: \" + official_neighbourhoods_byName[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs of: <br>\n",
    "how many were Nan vs how many were supposed to be nan <br>\n",
    "comparing what the distribution should have been (bar chart) /each bar is a neighbourhood <br>\n",
    "comparing what the distrubution actually was (bar chart) <br>\n",
    "list ones that were correctly identified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2380\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "total_nan = 0\n",
    "total_nan_test = 0\n",
    "for key in kNearest_650_1.keys():\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == False:\n",
    "        total_nan += 1\n",
    "    if isinstance(kNearest_650_1[key],str) == False:\n",
    "        total_nan_test += 1\n",
    "\n",
    "for key in kNearest_650_2.keys():\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == False:\n",
    "        total_nan += 1\n",
    "    if isinstance(kNearest_650_2[key],str) == False:\n",
    "        total_nan_test += 1\n",
    "\n",
    "for key in kNearest_650_3.keys():\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == False:\n",
    "        total_nan += 1\n",
    "    if isinstance(kNearest_650_3[key],str) == False:\n",
    "        total_nan_test += 1\n",
    "\n",
    "for key in kNearest_650_4.keys():\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == False:\n",
    "        total_nan += 1\n",
    "    if isinstance(kNearest_650_4[key],str) == False:\n",
    "        total_nan_test += 1\n",
    "print(total_nan)\n",
    "print(total_nan_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison Bar Chart information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For actual answers, distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 330165.69it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 330136.18it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 524908.87it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 330155.86it/s]\n"
     ]
    }
   ],
   "source": [
    "answerkey_neighbourhoods = []\n",
    "for key in tqdm(kNearest_650_1.keys()):\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        answerkey_neighbourhoods.append(official_neighbourhoods_byName[key])\n",
    "for key in tqdm(kNearest_650_2.keys()):\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        answerkey_neighbourhoods.append(official_neighbourhoods_byName[key])\n",
    "for key in tqdm(kNearest_650_3.keys()):\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        answerkey_neighbourhoods.append(official_neighbourhoods_byName[key])\n",
    "for key in tqdm(kNearest_650_4.keys()):\n",
    "    if isinstance(official_neighbourhoods_byName[key],str) == True:\n",
    "        answerkey_neighbourhoods.append(official_neighbourhoods_byName[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_answerkey = Counter(answerkey_neighbourhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counting_answerkey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the correct neigbourhoods, can see what the distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 17701.90it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 61286.93it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 71393.10it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 56213.99it/s]\n"
     ]
    }
   ],
   "source": [
    "correct_neighbourhoods = []\n",
    "for key in tqdm(kNearest_650_1.keys()):\n",
    "    if kNearest_650_1[key] == official_neighbourhoods_byName[key]:\n",
    "        correct_neighbourhoods.append(official_neighbourhoods_byName[key])\n",
    "for key in tqdm(kNearest_650_2.keys()):\n",
    "    if kNearest_650_2[key] == official_neighbourhoods_byName[key]:\n",
    "        correct_neighbourhoods.append(official_neighbourhoods_byName[key])\n",
    "for key in tqdm(kNearest_650_3.keys()):\n",
    "    if kNearest_650_3[key] == official_neighbourhoods_byName[key]:\n",
    "        correct_neighbourhoods.append(official_neighbourhoods_byName[key])\n",
    "for key in tqdm(kNearest_650_4.keys()):\n",
    "    if kNearest_650_4[key] == official_neighbourhoods_byName[key]:\n",
    "        correct_neighbourhoods.append(official_neighbourhoods_byName[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_correct = Counter(correct_neighbourhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counting_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test case neighbourhood answers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2642/2642 [00:00<00:00, 60030.61it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 293351.45it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 101596.66it/s]\n",
      "100%|██████████| 2642/2642 [00:00<00:00, 659839.89it/s]\n"
     ]
    }
   ],
   "source": [
    "test_neighbourhoods = []\n",
    "for key in tqdm(kNearest_650_1.keys()):\n",
    "    test_neighbourhoods.append(kNearest_650_1[key])\n",
    "for key in tqdm(kNearest_650_2.keys()):\n",
    "    test_neighbourhoods.append(kNearest_650_2[key])\n",
    "for key in tqdm(kNearest_650_3.keys()):\n",
    "    test_neighbourhoods.append(kNearest_650_3[key])\n",
    "for key in tqdm(kNearest_650_4.keys()):\n",
    "    test_neighbourhoods.append(kNearest_650_4[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "counting_test = Counter(test_neighbourhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counting_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
